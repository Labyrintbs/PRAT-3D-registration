{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_landmarks(txt_path):\n",
    "    \"\"\"\n",
    "    Load landmark coordinates exported from CloudCompare.\n",
    "    Supports both CSV (x,y,z) and space-separated formats.\n",
    "    Returns: (N,3) numpy array\n",
    "    \"\"\"\n",
    "    pts = []\n",
    "    with open(txt_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            if \",\" in line:\n",
    "                vals = line.split(\",\")\n",
    "            else:\n",
    "                vals = line.split()\n",
    "\n",
    "            if len(vals) < 3:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                x = float(vals[0])\n",
    "                y = float(vals[1])\n",
    "                z = float(vals[2])\n",
    "                pts.append([x, y, z])\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    return np.asarray(pts, dtype=np.float64)\n",
    "\n",
    "def compute_rigid_transform(P, Q):\n",
    "    \"\"\"\n",
    "    Given 2 list of points:\n",
    "      P: (N, 3) moving object points (mov_landmarks)\n",
    "      Q: (N, 3) reference object points (ref_landmarks)\n",
    "    Solve Rigid transformation T so that Q ≈ R @ P + t\n",
    "    Return:\n",
    "     4x4 transform matrix T\n",
    "    \"\"\"\n",
    "    assert P.shape == Q.shape\n",
    "    # Centroid\n",
    "    centroid_P = P.mean(axis=0)\n",
    "    centroid_Q = Q.mean(axis=0)\n",
    "\n",
    "    # Decentroid\n",
    "    P_centered = P - centroid_P\n",
    "    Q_centered = Q - centroid_Q\n",
    "\n",
    "    # Covariance\n",
    "    H = P_centered.T @ Q_centered\n",
    "\n",
    "    # SVD\n",
    "    U, S, Vt = np.linalg.svd(H)\n",
    "    R = Vt.T @ U.T\n",
    "\n",
    "    # Reflection case\n",
    "    if np.linalg.det(R) < 0:\n",
    "        Vt[2, :] *= -1\n",
    "        R = Vt.T @ U.T\n",
    "\n",
    "    # translation\n",
    "    t = centroid_Q - R @ centroid_P\n",
    "\n",
    "    T = np.eye(4)\n",
    "    T[:3, :3] = R\n",
    "    T[:3, 3] = t\n",
    "    return T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tuboshu/Documents/2025/M1/PRAT/Code\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PointCloud with 1349372 points.\n",
      "PointCloud with 1711170 points.\n",
      "Landmark pairs: (5, 3)\n"
     ]
    }
   ],
   "source": [
    "ref_pcd_path = \"../Data/ICP_test/ref_Tete_D.ply\"\n",
    "mov_pcd_path = \"../Data/ICP_test/move_Dragon_01_Transform.ply\"\n",
    "ref_lm_path  = \"../Data/ICP_test/picking_list_Tete_D.txt\"\n",
    "mov_lm_path  = \"../Data/ICP_test/picking_list_Dragon_01_Transform.txt\"\n",
    "\n",
    "# Read Point Cloud\n",
    "ref_pcd = o3d.io.read_point_cloud(ref_pcd_path)\n",
    "mov_pcd_raw = o3d.io.read_point_cloud(mov_pcd_path)\n",
    "\n",
    "print(ref_pcd)\n",
    "print(mov_pcd_raw)\n",
    "\n",
    "# Read landmarks\n",
    "Q = load_landmarks(ref_lm_path)  # reference\n",
    "P = load_landmarks(mov_lm_path)  # moving\n",
    "\n",
    "print(\"Landmark pairs:\", P.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial transform (from landmarks):\n",
      " [[ 9.62467722e-01  2.70397248e-01 -2.32639644e-02 -3.62131506e+01]\n",
      " [-2.70754231e-01  9.50764689e-01 -1.50793408e-01 -8.51340723e+02]\n",
      " [-1.86555667e-02  1.51432605e-01  9.88291524e-01 -2.97968502e+01]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_init = compute_rigid_transform(P, Q)\n",
    "print(\"Initial transform (from landmarks):\\n\", T_init)\n",
    "np.savetxt(\"T_landmark_4x4.txt\", T_init)\n",
    "\n",
    "\n",
    "# mov_pcd_landmark = mov_pcd.transform(T_init.copy())\n",
    "\n",
    "# o3d.io.write_point_cloud(\"../Data/ICP_test/dragon_after_landmark.ply\", mov_pcd_landmark)\n",
    "\n",
    "mov_after_landmark = copy.deepcopy(mov_pcd_raw)\n",
    "mov_after_landmark.transform(T_init)\n",
    "o3d.io.write_point_cloud(\"../Data/ICP_test/dragon_after_landmark.ply\", mov_after_landmark)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICP result:\n",
      "  fitness: 0.8167061133610337\n",
      "  inlier_rmse: 1.7607851593499075\n",
      "  T_icp:\n",
      " [[ 9.54561581e-01  2.96914653e-01 -2.55710273e-02 -4.80688633e+01]\n",
      " [-2.97020892e-01  9.40874457e-01 -1.62892129e-01 -9.31408116e+02]\n",
      " [-2.43059335e-02  1.63085697e-01  9.86312464e-01 -4.09150237e+01]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "max_corr_dist = 10.0\n",
    "\n",
    "result_icp = o3d.pipelines.registration.registration_icp(\n",
    "    mov_pcd_raw,                   # source\n",
    "    ref_pcd,                       # target\n",
    "    max_corr_dist,\n",
    "    T_init,                        # init_trans from Landmark coarse registration\n",
    "    o3d.pipelines.registration.TransformationEstimationPointToPoint(),\n",
    "    o3d.pipelines.registration.ICPConvergenceCriteria(\n",
    "        max_iteration=100\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"ICP result:\")\n",
    "print(\"  fitness:\", result_icp.fitness)\n",
    "print(\"  inlier_rmse:\", result_icp.inlier_rmse)\n",
    "print(\"  T_icp:\\n\", result_icp.transformation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved aligned point cloud to dragon_after_icp_python.ply\n",
      "Saved ICP transform to T_icp_4x4.txt\n"
     ]
    }
   ],
   "source": [
    "T_total = result_icp.transformation\n",
    "np.savetxt(\"T_icp_total_4x4.txt\", T_total)\n",
    "\n",
    "mov_after_icp = copy.deepcopy(mov_pcd_raw)\n",
    "mov_after_icp.transform(T_total)\n",
    "o3d.io.write_point_cloud(\"../Data/ICP_test/dragon_after_icp_python.ply\", mov_after_icp)\n",
    "\n",
    "\n",
    "print(\"Saved aligned point cloud to dragon_after_icp_python.ply\")\n",
    "print(\"Saved ICP transform to T_icp_4x4.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison\n",
    "Apply icp to mov dragon\n",
    "\n",
    "[11:24:13] [ComputeDistances] Mean distance = 11.4888 / std deviation = 26.862\n",
    "\n",
    "Default software algo:\n",
    "\n",
    "[11:58:09] [ComputeDistances] Mean distance = 11.5154 / std deviation = 26.9277"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FPFH + RANSAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_point_cloud(pcd: o3d.geometry.PointCloud, voxel_size):\n",
    "    \"\"\"\n",
    "    Downsample + normal estimation + FPFH feature extraction.\n",
    "\n",
    "    Returns:\n",
    "        pcd_down: downsampled point cloud with normals\n",
    "        fpfh:     o3d.pipelines.registration.Feature (shape: 33 x N)\n",
    "    \"\"\"\n",
    "    # Voxel downsample \n",
    "    pcd_down = pcd.voxel_down_sample(voxel_size)\n",
    "\n",
    "    # Remove statistical outliers\n",
    "    # Considered as outlier if avg dist among neighbors >= global avg dist + std_ratio * std\n",
    "    pcd_down, _ = pcd_down.remove_statistical_outlier(nb_neighbors=20, std_ratio=2.0)\n",
    "\n",
    "    # Estimate normals\n",
    "    radius_normal = voxel_size * 2.0\n",
    "    pcd_down.estimate_normals(\n",
    "        o3d.geometry.KDTreeSearchParamHybrid(radius=radius_normal, max_nn=30)\n",
    "    )\n",
    "\n",
    "    # Compute FPFH feature\n",
    "    radius_feature = voxel_size * 5.0\n",
    "    fpfh = o3d.pipelines.registration.compute_fpfh_feature(\n",
    "        pcd_down,\n",
    "        o3d.geometry.KDTreeSearchParamHybrid(radius=radius_feature, max_nn=100)\n",
    "    )\n",
    "\n",
    "    return pcd_down, fpfh\n",
    "\n",
    "\n",
    "def draw_registration_result(source, target, transformation):\n",
    "    \"\"\"Visualize alignment with two colors.\"\"\"\n",
    "    source_temp = copy.deepcopy(source)\n",
    "    target_temp = copy.deepcopy(target)\n",
    "    source_temp.paint_uniform_color([1.0, 0.0, 0.0])  # red\n",
    "    target_temp.paint_uniform_color([0.0, 0.5, 1.0])  # blue\n",
    "    source_temp.transform(transformation)\n",
    "    o3d.visualization.draw_geometries([source_temp, target_temp])\n",
    "\n",
    "\n",
    "def global_registration_ransac(src_down, tgt_down, src_fpfh, tgt_fpfh, voxel_size: float):\n",
    "    \"\"\"\n",
    "    RANSAC-based global registration on FPFH feature matches.\n",
    "    \"\"\"\n",
    "    distance_threshold = voxel_size * 1.5\n",
    "\n",
    "    result = o3d.pipelines.registration.registration_ransac_based_on_feature_matching(\n",
    "        src_down, tgt_down,\n",
    "        src_fpfh, tgt_fpfh,\n",
    "        mutual_filter=True, # ensure src/tar NN corespondance\n",
    "        max_correspondence_distance=distance_threshold,\n",
    "        estimation_method=o3d.pipelines.registration.TransformationEstimationPointToPoint(False), # rigid transform\n",
    "        ransac_n=4, # sample point pairs to solve T \n",
    "        checkers=[ # pre filter\n",
    "            # Enforce similar edge length ratios to reject degenerate matches\n",
    "            o3d.pipelines.registration.CorrespondenceCheckerBasedOnEdgeLength(0.9),\n",
    "            # Reject matches with too large geometric distance\n",
    "            o3d.pipelines.registration.CorrespondenceCheckerBasedOnDistance(distance_threshold),\n",
    "        ],\n",
    "        criteria=o3d.pipelines.registration.RANSACConvergenceCriteria(\n",
    "            max_iteration=100000,  \n",
    "            confidence=0.999\n",
    "        )\n",
    "    )\n",
    "    return result\n",
    "\n",
    "\n",
    "def refine_registration_icp(src_full, tgt_full, init_T, voxel_size, use_point_to_plane=False):\n",
    "    \"\"\"\n",
    "    ICP refinement (point-to-plane is usually better if normals are reliable).\n",
    "    \"\"\"\n",
    "    # Set a tighter threshold than RANSAC\n",
    "    max_corr_dist = voxel_size * 1.0\n",
    "\n",
    "    if use_point_to_plane:\n",
    "        # Normals are required for point-to-plane ICP (target normals are used)\n",
    "        tgt_full.estimate_normals(\n",
    "            o3d.geometry.KDTreeSearchParamHybrid(radius=voxel_size * 2.0, max_nn=30)\n",
    "        )\n",
    "        src_full.estimate_normals(\n",
    "            o3d.geometry.KDTreeSearchParamHybrid(radius=voxel_size * 2.0, max_nn=30)\n",
    "        )\n",
    "        estimation = o3d.pipelines.registration.TransformationEstimationPointToPlane()\n",
    "    else:\n",
    "        estimation = o3d.pipelines.registration.TransformationEstimationPointToPoint()\n",
    "\n",
    "    result_icp = o3d.pipelines.registration.registration_icp(\n",
    "        src_full, tgt_full,\n",
    "        max_corr_dist,\n",
    "        init_T,\n",
    "        estimation_method=estimation,\n",
    "        criteria=o3d.pipelines.registration.ICPConvergenceCriteria(max_iteration=80)\n",
    "    )\n",
    "    return result_icp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded:\n",
      "  ref: PointCloud with 1349372 points.\n",
      "  mov: PointCloud with 1711170 points.\n",
      "Downsampled:\n",
      "  ref_down: PointCloud with 57450 points.\n",
      "  mov_down: PointCloud with 59566 points.\n",
      "FPFH dims: (33, 57450) (33, 59566)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ref_path = \"../Data/ICP_test/ref_Tete_D.ply\"\n",
    "mov_path = \"../Data/ICP_test/move_Dragon_01_Transform.ply\"\n",
    "\n",
    "ref_pcd = o3d.io.read_point_cloud(ref_path)\n",
    "mov_pcd = o3d.io.read_point_cloud(mov_path)\n",
    "\n",
    "print(\"Loaded:\")\n",
    "print(\"  ref:\", ref_pcd)\n",
    "print(\"  mov:\", mov_pcd)\n",
    "\n",
    "\n",
    "voxel_size = 5.0\n",
    "\n",
    "# 1) Preprocess for FPFH + RANSAC on downsampled point clouds\n",
    "ref_down, ref_fpfh = preprocess_point_cloud(ref_pcd, voxel_size)\n",
    "mov_down, mov_fpfh = preprocess_point_cloud(mov_pcd, voxel_size)\n",
    "\n",
    "print(\"Downsampled:\")\n",
    "print(\"  ref_down:\", ref_down)\n",
    "print(\"  mov_down:\", mov_down)\n",
    "print(\"FPFH dims:\", ref_fpfh.data.shape, mov_fpfh.data.shape)  # (33, N)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANSAC result:\n",
      "  fitness: 0.7830305879192828\n",
      "  inlier_rmse: 2.664491435886807\n",
      "  T_ransac:\n",
      " [[ 9.55147585e-01  2.95106543e-01 -2.46011912e-02 -4.31887167e+01]\n",
      " [-2.95069755e-01  9.41413079e-01 -1.63325612e-01 -9.31767843e+02]\n",
      " [-2.50385735e-02  1.63259131e-01  9.86265444e-01 -4.12658185e+01]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# 2) Global registration (RANSAC)\n",
    "result_ransac = global_registration_ransac(mov_down, ref_down, mov_fpfh, ref_fpfh, voxel_size)\n",
    "print(\"\\nRANSAC result:\")\n",
    "print(\"  fitness:\", result_ransac.fitness)\n",
    "print(\"  inlier_rmse:\", result_ransac.inlier_rmse)\n",
    "print(\"  T_ransac:\\n\", result_ransac.transformation)\n",
    "\n",
    "# Visualize coarse alignment\n",
    "draw_registration_result(mov_down, ref_down, result_ransac.transformation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ICP result:\n",
      "  fitness: 0.7907022680388273\n",
      "  inlier_rmse: 1.2211446515516153\n",
      "  T_icp:\n",
      " [[ 9.54329399e-01  2.97668491e-01 -2.54728865e-02 -4.74875924e+01]\n",
      " [-2.97744996e-01  9.40625150e-01 -1.63009950e-01 -9.32482530e+02]\n",
      " [-2.45624882e-02  1.63149612e-01  9.86295538e-01 -4.11525286e+01]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# 3) ICP refinement on full-resolution clouds (or you can use downsampled first)\n",
    "result_icp = refine_registration_icp(mov_pcd, ref_pcd, result_ransac.transformation, voxel_size, use_point_to_plane=True)\n",
    "print(\"\\nICP result:\")\n",
    "print(\"  fitness:\", result_icp.fitness)\n",
    "print(\"  inlier_rmse:\", result_icp.inlier_rmse)\n",
    "print(\"  T_icp:\\n\", result_icp.transformation)\n",
    "\n",
    "# Visualize refined alignment\n",
    "draw_registration_result(mov_pcd, ref_pcd, result_icp.transformation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved: mov_aligned_by_fpfh_ransac_icp_p2p.ply\n"
     ]
    }
   ],
   "source": [
    "# 4) Save transformed moving point cloud (optional)\n",
    "mov_aligned = copy.deepcopy(mov_pcd)\n",
    "mov_aligned.transform(result_icp.transformation)\n",
    "o3d.io.write_point_cloud(\"../Data/FPFH/mov_aligned_by_fpfh_ransac_icp_p2p.ply\", mov_aligned)\n",
    "print(\"\\nSaved:\", \"mov_aligned_by_fpfh_ransac_icp_p2p.ply\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison\n",
    "[16:12:34] [ComputeDistances] Mean distance = 11.5045 / std deviation = 26.9021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nn_distances(source: o3d.geometry.PointCloud, target: o3d.geometry.PointCloud) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute nearest neighbor distances from each point in 'source' to 'target'.\n",
    "    Returns:\n",
    "        dists: (N,) array of Euclidean distances.\n",
    "    \"\"\"\n",
    "    target_kd = o3d.geometry.KDTreeFlann(target)\n",
    "    src_pts = np.asarray(source.points)\n",
    "\n",
    "    dists = np.empty(len(src_pts), dtype=np.float64)\n",
    "    for i, p in enumerate(src_pts):\n",
    "        # 1-NN search\n",
    "        _, idx, dist2 = target_kd.search_knn_vector_3d(p, 1)\n",
    "        dists[i] = np.sqrt(dist2[0])\n",
    "    return dists\n",
    "\n",
    "\n",
    "def registration_metrics(source_aligned: o3d.geometry.PointCloud,\n",
    "                         target: o3d.geometry.PointCloud,\n",
    "                         thresholds=(5.0, 10.0),\n",
    "                         percentiles=(50, 90, 95)) -> dict:\n",
    "    \"\"\"\n",
    "    Compute overlap-aware metrics for registration evaluation.\n",
    "\n",
    "    Metrics:\n",
    "      - median / P90 / P95 distances\n",
    "      - coverage@tau: ratio of points with dist < tau\n",
    "      - trimmed_rmse@tau: RMSE computed only on points with dist < tau\n",
    "      - mean / std (for reference)\n",
    "    \"\"\"\n",
    "    d = nn_distances(source_aligned, target)\n",
    "\n",
    "    out = {\n",
    "        \"mean\": float(d.mean()),\n",
    "        \"std\": float(d.std()),\n",
    "        \"median\": float(np.percentile(d, 50)),\n",
    "    }\n",
    "    for p in percentiles:\n",
    "        out[f\"p{p}\"] = float(np.percentile(d, p))\n",
    "\n",
    "    for tau in thresholds:\n",
    "        inliers = d < tau\n",
    "        coverage = float(inliers.mean())\n",
    "        if inliers.any():\n",
    "            trimmed_rmse = float(np.sqrt(np.mean(d[inliers] ** 2)))\n",
    "            trimmed_mean = float(d[inliers].mean())\n",
    "        else:\n",
    "            trimmed_rmse = float(\"nan\")\n",
    "            trimmed_mean = float(\"nan\")\n",
    "        out[f\"coverage@{tau}\"] = coverage\n",
    "        out[f\"trimmed_mean@{tau}\"] = trimmed_mean\n",
    "        out[f\"trimmed_rmse@{tau}\"] = trimmed_rmse\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def symmetric_chamfer(source_aligned: o3d.geometry.PointCloud,\n",
    "                      target: o3d.geometry.PointCloud) -> float:\n",
    "    \"\"\"\n",
    "    Symmetric Chamfer distance (mean NN distance both directions).\n",
    "    \"\"\"\n",
    "    d_st = nn_distances(source_aligned, target).mean()\n",
    "    d_ts = nn_distances(target, source_aligned).mean()\n",
    "    return float(d_st + d_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original not registered evaluation\n",
      "{'mean': 28.790589237437796, 'std': 21.611957785388448, 'median': 24.060411166186718, 'p50': 24.060411166186718, 'p90': 61.29547679266786, 'p95': 71.37153082536858, 'coverage@5.0': 0.1196292595124973, 'trimmed_mean@5.0': 2.4777668386423106, 'trimmed_rmse@5.0': 2.83796814333311, 'coverage@10.0': 0.22594189940216344, 'trimmed_mean@10.0': 4.844387508719292, 'trimmed_rmse@10.0': 5.637451842177725}\n",
      "Symmetric Chamfer: 54.16179091590028\n"
     ]
    }
   ],
   "source": [
    "ref_path = \"../Data/ICP_test/ref_Tete_D.ply\"\n",
    "mov_path = \"../Data/ICP_test/move_Dragon_01_Transform.ply\"\n",
    "src = o3d.io.read_point_cloud(mov_path)\n",
    "tgt = o3d.io.read_point_cloud(ref_path)\n",
    "\n",
    "# src_aligned = copy.deepcopy(src)\n",
    "# src_aligned.transform(T_total)\n",
    "\n",
    "metrics = registration_metrics(src, tgt, thresholds=(5.0, 10.0))\n",
    "print(\"Original not registered evaluation\")\n",
    "print(metrics)\n",
    "\n",
    "print(\"Symmetric Chamfer:\", symmetric_chamfer(src, tgt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual landmark + ICP evaluation\n",
      "{'mean': 11.900873394379946, 'std': 26.68532721206653, 'median': 0.8916812282821849, 'p50': 0.8916812282821849, 'p90': 52.385716913856236, 'p95': 79.81466731250123, 'coverage@5.0': 0.7907560324222608, 'trimmed_mean@5.0': 0.9568073840270434, 'trimmed_rmse@5.0': 1.224656404608553, 'coverage@10.0': 0.8167061133610337, 'trimmed_mean@10.0': 1.1507352380483076, 'trimmed_rmse@10.0': 1.7607851593499064}\n",
      "Symmetric Chamfer: 16.668003987122248\n"
     ]
    }
   ],
   "source": [
    "ref_path = \"../Data/ICP_test/ref_Tete_D.ply\"\n",
    "mov_path = \"../Data/ICP_test/dragon_after_icp_python.ply\"\n",
    "src = o3d.io.read_point_cloud(mov_path)\n",
    "tgt = o3d.io.read_point_cloud(ref_path)\n",
    "\n",
    "# src_aligned = copy.deepcopy(src)\n",
    "# src_aligned.transform(T_total)\n",
    "\n",
    "metrics = registration_metrics(src, tgt, thresholds=(5.0, 10.0))\n",
    "print(\"Manual landmark + ICP evaluation\")\n",
    "print(metrics)\n",
    "\n",
    "print(\"Symmetric Chamfer:\", symmetric_chamfer(src, tgt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPFH + RANSAC + ICP evaluation\n",
      "{'mean': 11.919651183532961, 'std': 26.723596740054912, 'median': 0.8878449859671382, 'p50': 0.8878449859671382, 'p90': 52.503750839545056, 'p95': 79.93291604220614, 'coverage@5.0': 0.7907022680388273, 'trimmed_mean@5.0': 0.953115089069285, 'trimmed_rmse@5.0': 1.2211446515516156, 'coverage@10.0': 0.8165477421880936, 'trimmed_mean@10.0': 1.1466224471712876, 'trimmed_rmse@10.0': 1.7575508328904914}\n",
      "Symmetric Chamfer: 16.678766929614426\n"
     ]
    }
   ],
   "source": [
    "ref_path = \"../Data/ICP_test/ref_Tete_D.ply\"\n",
    "mov_path = \"../Data/FPFH/mov_aligned_by_fpfh_ransac_icp_p2p.ply\"\n",
    "src = o3d.io.read_point_cloud(mov_path)\n",
    "tgt = o3d.io.read_point_cloud(ref_path)\n",
    "\n",
    "# src_aligned = copy.deepcopy(src)\n",
    "# src_aligned.transform(T_total)\n",
    "\n",
    "metrics = registration_metrics(src, tgt, thresholds=(5.0, 10.0))\n",
    "print(\"FPFH + RANSAC + ICP evaluation\")\n",
    "print(metrics)\n",
    "\n",
    "print(\"Symmetric Chamfer:\", symmetric_chamfer(src, tgt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpinNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pointnet2_ops'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/tuboshu/Documents/2025/M1/PRAT/Code/experiment.ipynb Cell 26\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tuboshu/Documents/2025/M1/PRAT/Code/experiment.ipynb#X34sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39msys\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m \u001b[39m\u001b[39mos\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tuboshu/Documents/2025/M1/PRAT/Code/experiment.ipynb#X34sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39minsert(\u001b[39m0\u001b[39m, os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mabspath(\u001b[39m\"\u001b[39m\u001b[39m./SpinNet\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tuboshu/Documents/2025/M1/PRAT/Code/experiment.ipynb#X34sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mnetwork\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mSpinNet\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m Descriptor_Net\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tuboshu/Documents/2025/M1/PRAT/Code/experiment.ipynb#X34sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mbuild_spinnet_model\u001b[39m(ckpt_path: \u001b[39mstr\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tuboshu/Documents/2025/M1/PRAT/Code/experiment.ipynb#X34sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m                         des_r\u001b[39m=\u001b[39m\u001b[39m0.30\u001b[39m, rad_n\u001b[39m=\u001b[39m\u001b[39m9\u001b[39m, azi_n\u001b[39m=\u001b[39m\u001b[39m80\u001b[39m, ele_n\u001b[39m=\u001b[39m\u001b[39m40\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tuboshu/Documents/2025/M1/PRAT/Code/experiment.ipynb#X34sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m                         voxel_r\u001b[39m=\u001b[39m\u001b[39m0.04\u001b[39m, voxel_sample\u001b[39m=\u001b[39m\u001b[39m30\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tuboshu/Documents/2025/M1/PRAT/Code/experiment.ipynb#X34sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m                         dataset\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m3DMatch\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tuboshu/Documents/2025/M1/PRAT/Code/experiment.ipynb#X34sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m                         device\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tuboshu/Documents/2025/M1/PRAT/Code/experiment.ipynb#X34sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     model \u001b[39m=\u001b[39m Descriptor_Net(des_r, rad_n, azi_n, ele_n, voxel_r, voxel_sample, dataset)\n",
      "File \u001b[0;32m~/Documents/2025/M1/PRAT/Code/SpinNet/network/SpinNet.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mas\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mnn\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mas\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mF\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mnetwork\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mThreeDCCN\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mas\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mpn\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mscript\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommon\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mas\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcm\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mscript\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommon\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m switch\n",
      "File \u001b[0;32m~/Documents/2025/M1/PRAT/Code/SpinNet/network/ThreeDCCN.py:5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mas\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mnn\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mas\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mF\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mscript\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommon\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mas\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcm\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mclass\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mBaseNet\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n\u001b[1;32m      9\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" Takes a list of images as input, and returns for each image:\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m        - a pixelwise descriptor\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39m        - a pixelwise confidence\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/2025/M1/PRAT/Code/SpinNet/script/common.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mneighbors\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m KDTree\n\u001b[0;32m----> 7\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mpointnet2_ops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpointnet2_utils\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mas\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mpnt2\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mas\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mF\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mautograd\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m Variable\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pointnet2_ops'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(\"./SpinNet\"))\n",
    "from network.SpinNet import Descriptor_Net\n",
    "\n",
    "def build_spinnet_model(ckpt_path: str,\n",
    "                        des_r=0.30, rad_n=9, azi_n=80, ele_n=40,\n",
    "                        voxel_r=0.04, voxel_sample=30,\n",
    "                        dataset='3DMatch',\n",
    "                        device='cuda'):\n",
    "    model = Descriptor_Net(des_r, rad_n, azi_n, ele_n, voxel_r, voxel_sample, dataset)\n",
    "    sd = torch.load(ckpt_path, map_location=device)\n",
    "    model.load_state_dict(sd)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "def spinnet_features_for_pcd(pcd_down: o3d.geometry.PointCloud,\n",
    "                             model: torch.nn.Module,\n",
    "                             patch_radius: float,\n",
    "                             N: int = 2048,\n",
    "                             batch_size: int = 64,\n",
    "                             device: str = 'cuda'):\n",
    "    \"\"\"\n",
    "    For each point in pcd_down, build a local patch (N,3), run SpinNet, return Open3D Feature (32, num_pts).\n",
    "    \"\"\"\n",
    "    pts = np.asarray(pcd_down.points).astype(np.float32)\n",
    "    num_pts = pts.shape[0]\n",
    "\n",
    "    # KDTree for neighborhood query\n",
    "    kdtree = o3d.geometry.KDTreeFlann(pcd_down)\n",
    "\n",
    "    desc_list = []\n",
    "    # Build patches in chunks\n",
    "    patches_buf = []\n",
    "    idx_buf = []\n",
    "\n",
    "    for i in range(num_pts):\n",
    "        center = pts[i]\n",
    "\n",
    "        # radius search\n",
    "        _, idxs, _ = kdtree.search_radius_vector_3d(center, patch_radius)\n",
    "        if len(idxs) < 5:\n",
    "            # fallback: if neighborhood is too small, just repeat center\n",
    "            patch = np.repeat(center[None, :], N, axis=0)\n",
    "        else:\n",
    "            neigh = pts[np.asarray(idxs, dtype=np.int64)]\n",
    "\n",
    "            # sample to fixed N\n",
    "            if neigh.shape[0] >= N:\n",
    "                sel = np.random.choice(neigh.shape[0], N, replace=False)\n",
    "                patch = neigh[sel]\n",
    "            else:\n",
    "                # pad by resampling with replacement\n",
    "                sel = np.random.choice(neigh.shape[0], N, replace=True)\n",
    "                patch = neigh[sel]\n",
    "\n",
    "            # IMPORTANT: ensure the last point is the center (SpinNet uses input[:, -1, :] as center)\n",
    "            patch[-1] = center\n",
    "\n",
    "        patches_buf.append(patch)\n",
    "        idx_buf.append(i)\n",
    "\n",
    "        # run a batch\n",
    "        if len(patches_buf) == batch_size or i == num_pts - 1:\n",
    "            batch = torch.from_numpy(np.stack(patches_buf, axis=0)).to(device=device, dtype=torch.float32)  # (B,N,3)\n",
    "            with torch.no_grad():\n",
    "                out = model(batch)                   # (B,32,1,1)\n",
    "                out = out.view(out.shape[0], -1)     # (B,32)\n",
    "                out = F.normalize(out, p=2, dim=1)   # (B,32)\n",
    "            desc_list.append(out.detach().cpu().numpy())\n",
    "            patches_buf = []\n",
    "            idx_buf = []\n",
    "\n",
    "    desc = np.concatenate(desc_list, axis=0)  # (num_pts, 32)\n",
    "\n",
    "    feat = o3d.pipelines.registration.Feature()\n",
    "    feat.data = desc.T  # Open3D expects (dim, num_pts)\n",
    "    return feat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "\n",
    "ref_path = \"../Data/ICP_test/ref_Tete_D.ply\"\n",
    "mov_path = \"../Data/ICP_test/move_Dragon_01_Transform.ply\"\n",
    "\n",
    "ref_pcd = o3d.io.read_point_cloud(ref_path)\n",
    "mov_pcd = o3d.io.read_point_cloud(mov_path)\n",
    "\n",
    "bbox = ref_pcd.get_axis_aligned_bounding_box()\n",
    "diag = np.linalg.norm(bbox.get_extent())\n",
    "print(\"diag =\", diag) # Verify the len unity\n",
    "\n",
    "# 0) 统一单位：假设输入是 mm -> m\n",
    "scale = 1e-3\n",
    "ref_pcd.scale(scale, center=ref_pcd.get_center())\n",
    "mov_pcd.scale(scale, center=mov_pcd.get_center())\n",
    "\n",
    "# 1) downsample（单位：m）\n",
    "voxel_size = 0.005  # 5mm = 0.005m\n",
    "ref_down = ref_pcd.voxel_down_sample(voxel_size)\n",
    "mov_down = mov_pcd.voxel_down_sample(voxel_size)\n",
    "\n",
    "ref_down, _ = ref_down.remove_statistical_outlier(nb_neighbors=20, std_ratio=2.0)\n",
    "mov_down, _ = mov_down.remove_statistical_outlier(nb_neighbors=20, std_ratio=2.0)\n",
    "\n",
    "# 2) SpinNet model（你在 Mac 上就用 cpu）\n",
    "ckpt_path = \"pre-trained_models/3DMatch_best.pkl\"\n",
    "model = build_spinnet_model(\n",
    "    ckpt_path,\n",
    "    des_r=0.30, rad_n=9, azi_n=80, ele_n=40,\n",
    "    voxel_r=0.04, voxel_sample=30,\n",
    "    dataset=\"3DMatch\",\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "# 3) SpinNet feature（patch_radius：m）\n",
    "ref_feat = spinnet_features_for_pcd(ref_down, model, patch_radius=0.30, N=2048, batch_size=16, device=\"cpu\")\n",
    "mov_feat = spinnet_features_for_pcd(mov_down, model, patch_radius=0.30, N=2048, batch_size=16, device=\"cpu\")\n",
    "\n",
    "# 4) RANSAC + ICP（voxel_size：m）\n",
    "result_ransac = global_registration_ransac(mov_down, ref_down, mov_feat, ref_feat, voxel_size)\n",
    "result_icp = refine_registration_icp(mov_pcd, ref_pcd, result_ransac.transformation, voxel_size, use_point_to_plane=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) downsample 仍然保留（RANSAC/ICP 都需要）\n",
    "\n",
    "\n",
    "ref_path = \"../Data/ICP_test/ref_Tete_D.ply\"\n",
    "mov_path = \"../Data/ICP_test/move_Dragon_01_Transform.ply\"\n",
    "\n",
    "ref_pcd = o3d.io.read_point_cloud(ref_path)\n",
    "mov_pcd = o3d.io.read_point_cloud(mov_path)\n",
    "\n",
    "\n",
    "voxel_size = 5.0\n",
    "ref_down = ref_pcd.voxel_down_sample(voxel_size)\n",
    "mov_down = mov_pcd.voxel_down_sample(voxel_size)\n",
    "\n",
    "# 可选：去外点/估计法向（ICP point-to-plane 会用到）\n",
    "ref_down, _ = ref_down.remove_statistical_outlier(nb_neighbors=20, std_ratio=2.0)\n",
    "mov_down, _ = mov_down.remove_statistical_outlier(nb_neighbors=20, std_ratio=2.0)\n",
    "\n",
    "# 2) SpinNet model\n",
    "ckpt_path = \"pre-trained_models/3DMatch_best.pkl\"   # 你仓库里的路径\n",
    "model = build_spinnet_model(ckpt_path,\n",
    "                            des_r=0.30, rad_n=9, azi_n=80, ele_n=40,\n",
    "                            voxel_r=0.04, voxel_sample=30,\n",
    "                            dataset=\"3DMatch\",\n",
    "                            device=\"cuda\")\n",
    "\n",
    "# 3) 用 SpinNet 算 feature（返回 Open3D Feature）\n",
    "# 注意：patch_radius 需要和你的点云单位一致。\n",
    "# 如果你的数据是 mm，并且 voxel_size=5.0(mm)，那 des_r=0.30(m) 就不对：\n",
    "#   - 要么把点云缩放到米\n",
    "#   - 要么把 patch_radius 改成 300.0(mm)\n",
    "ref_feat = spinnet_features_for_pcd(ref_down, model, patch_radius=300.0, N=2048, batch_size=64, device=\"cuda\")\n",
    "mov_feat = spinnet_features_for_pcd(mov_down, model, patch_radius=300.0, N=2048, batch_size=64, device=\"cuda\")\n",
    "\n",
    "# 4) RANSAC + ICP（你的函数可以不动，只是特征换了）\n",
    "result_ransac = global_registration_ransac(mov_down, ref_down, mov_feat, ref_feat, voxel_size)\n",
    "result_icp = refine_registration_icp(mov_pcd, ref_pcd, result_ransac.transformation, voxel_size, use_point_to_plane=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open3D",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
