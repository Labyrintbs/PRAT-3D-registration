{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_landmarks(txt_path):\n",
    "    \"\"\"\n",
    "    Load landmark coordinates exported from CloudCompare.\n",
    "    Supports both CSV (x,y,z) and space-separated formats.\n",
    "    Returns: (N,3) numpy array\n",
    "    \"\"\"\n",
    "    pts = []\n",
    "    with open(txt_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            if \",\" in line:\n",
    "                vals = line.split(\",\")\n",
    "            else:\n",
    "                vals = line.split()\n",
    "\n",
    "            if len(vals) < 3:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                x = float(vals[0])\n",
    "                y = float(vals[1])\n",
    "                z = float(vals[2])\n",
    "                pts.append([x, y, z])\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    return np.asarray(pts, dtype=np.float64)\n",
    "\n",
    "def compute_rigid_transform(P, Q):\n",
    "    \"\"\"\n",
    "    Given 2 list of points:\n",
    "      P: (N, 3) moving object points (mov_landmarks)\n",
    "      Q: (N, 3) reference object points (ref_landmarks)\n",
    "    Solve Rigid transformation T so that Q ≈ R @ P + t\n",
    "    Return:\n",
    "     4x4 transform matrix T\n",
    "    \"\"\"\n",
    "    assert P.shape == Q.shape\n",
    "    # Centroid\n",
    "    centroid_P = P.mean(axis=0)\n",
    "    centroid_Q = Q.mean(axis=0)\n",
    "\n",
    "    # Decentroid\n",
    "    P_centered = P - centroid_P\n",
    "    Q_centered = Q - centroid_Q\n",
    "\n",
    "    # Covariance\n",
    "    H = P_centered.T @ Q_centered\n",
    "\n",
    "    # SVD\n",
    "    U, S, Vt = np.linalg.svd(H)\n",
    "    R = Vt.T @ U.T\n",
    "\n",
    "    # Reflection case\n",
    "    if np.linalg.det(R) < 0:\n",
    "        Vt[2, :] *= -1\n",
    "        R = Vt.T @ U.T\n",
    "\n",
    "    # translation\n",
    "    t = centroid_Q - R @ centroid_P\n",
    "\n",
    "    T = np.eye(4)\n",
    "    T[:3, :3] = R\n",
    "    T[:3, 3] = t\n",
    "    return T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/users/nfs/Etu0/21400500/PRAT\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PointCloud with 1349372 points.\n",
      "PointCloud with 1711170 points.\n",
      "Landmark pairs: (5, 3)\n"
     ]
    }
   ],
   "source": [
    "ref_pcd_path = \"Data/ICP_test/ref_Tete_D.ply\"\n",
    "mov_pcd_path = \"Data/ICP_test/move_Dragon_01_Transform.ply\"\n",
    "ref_lm_path  = \"Data/ICP_test/picking_list_Tete_D.txt\"\n",
    "mov_lm_path  = \"Data/ICP_test/picking_list_Dragon_01_Transform.txt\"\n",
    "\n",
    "# Read Point Cloud\n",
    "ref_pcd = o3d.io.read_point_cloud(ref_pcd_path)\n",
    "mov_pcd_raw = o3d.io.read_point_cloud(mov_pcd_path)\n",
    "\n",
    "print(ref_pcd)\n",
    "print(mov_pcd_raw)\n",
    "\n",
    "# Read landmarks\n",
    "Q = load_landmarks(ref_lm_path)  # reference\n",
    "P = load_landmarks(mov_lm_path)  # moving\n",
    "\n",
    "print(\"Landmark pairs:\", P.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial transform (from landmarks):\n",
      " [[ 9.62467722e-01  2.70397248e-01 -2.32639644e-02 -3.62131506e+01]\n",
      " [-2.70754231e-01  9.50764689e-01 -1.50793408e-01 -8.51340723e+02]\n",
      " [-1.86555667e-02  1.51432605e-01  9.88291524e-01 -2.97968502e+01]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_init = compute_rigid_transform(P, Q)\n",
    "print(\"Initial transform (from landmarks):\\n\", T_init)\n",
    "np.savetxt(\"T_landmark_4x4.txt\", T_init)\n",
    "\n",
    "\n",
    "# mov_pcd_landmark = mov_pcd.transform(T_init.copy())\n",
    "\n",
    "# o3d.io.write_point_cloud(\"../Data/ICP_test/dragon_after_landmark.ply\", mov_pcd_landmark)\n",
    "\n",
    "mov_after_landmark = copy.deepcopy(mov_pcd_raw)\n",
    "mov_after_landmark.transform(T_init)\n",
    "o3d.io.write_point_cloud(\"../Data/ICP_test/dragon_after_landmark.ply\", mov_after_landmark)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICP result:\n",
      "  fitness: 0.8167061133610337\n",
      "  inlier_rmse: 1.7607851593499075\n",
      "  T_icp:\n",
      " [[ 9.54561581e-01  2.96914653e-01 -2.55710273e-02 -4.80688633e+01]\n",
      " [-2.97020892e-01  9.40874457e-01 -1.62892129e-01 -9.31408116e+02]\n",
      " [-2.43059335e-02  1.63085697e-01  9.86312464e-01 -4.09150237e+01]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "max_corr_dist = 10.0\n",
    "\n",
    "result_icp = o3d.pipelines.registration.registration_icp(\n",
    "    mov_pcd_raw,                   # source\n",
    "    ref_pcd,                       # target\n",
    "    max_corr_dist,\n",
    "    T_init,                        # init_trans from Landmark coarse registration\n",
    "    o3d.pipelines.registration.TransformationEstimationPointToPoint(),\n",
    "    o3d.pipelines.registration.ICPConvergenceCriteria(\n",
    "        max_iteration=100\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"ICP result:\")\n",
    "print(\"  fitness:\", result_icp.fitness)\n",
    "print(\"  inlier_rmse:\", result_icp.inlier_rmse)\n",
    "print(\"  T_icp:\\n\", result_icp.transformation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved aligned point cloud to dragon_after_icp_python.ply\n",
      "Saved ICP transform to T_icp_4x4.txt\n"
     ]
    }
   ],
   "source": [
    "T_total = result_icp.transformation\n",
    "np.savetxt(\"T_icp_total_4x4.txt\", T_total)\n",
    "\n",
    "mov_after_icp = copy.deepcopy(mov_pcd_raw)\n",
    "mov_after_icp.transform(T_total)\n",
    "o3d.io.write_point_cloud(\"../Data/ICP_test/dragon_after_icp_python.ply\", mov_after_icp)\n",
    "\n",
    "\n",
    "print(\"Saved aligned point cloud to dragon_after_icp_python.ply\")\n",
    "print(\"Saved ICP transform to T_icp_4x4.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison\n",
    "Apply icp to mov dragon\n",
    "\n",
    "[11:24:13] [ComputeDistances] Mean distance = 11.4888 / std deviation = 26.862\n",
    "\n",
    "Default software algo:\n",
    "\n",
    "[11:58:09] [ComputeDistances] Mean distance = 11.5154 / std deviation = 26.9277"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FPFH + RANSAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_point_cloud(pcd: o3d.geometry.PointCloud, voxel_size):\n",
    "    \"\"\"\n",
    "    Downsample + normal estimation + FPFH feature extraction.\n",
    "\n",
    "    Returns:\n",
    "        pcd_down: downsampled point cloud with normals\n",
    "        fpfh:     o3d.pipelines.registration.Feature (shape: 33 x N)\n",
    "    \"\"\"\n",
    "    # Voxel downsample \n",
    "    pcd_down = pcd.voxel_down_sample(voxel_size)\n",
    "\n",
    "    # Remove statistical outliers\n",
    "    # Considered as outlier if avg dist among neighbors >= global avg dist + std_ratio * std\n",
    "    pcd_down, _ = pcd_down.remove_statistical_outlier(nb_neighbors=20, std_ratio=2.0)\n",
    "\n",
    "    # Estimate normals\n",
    "    radius_normal = voxel_size * 2.0\n",
    "    pcd_down.estimate_normals(\n",
    "        o3d.geometry.KDTreeSearchParamHybrid(radius=radius_normal, max_nn=30)\n",
    "    )\n",
    "\n",
    "    # Compute FPFH feature\n",
    "    radius_feature = voxel_size * 5.0\n",
    "    fpfh = o3d.pipelines.registration.compute_fpfh_feature(\n",
    "        pcd_down,\n",
    "        o3d.geometry.KDTreeSearchParamHybrid(radius=radius_feature, max_nn=100)\n",
    "    )\n",
    "\n",
    "    return pcd_down, fpfh\n",
    "\n",
    "\n",
    "def draw_registration_result(source, target, transformation):\n",
    "    \"\"\"Visualize alignment with two colors.\"\"\"\n",
    "    source_temp = copy.deepcopy(source)\n",
    "    target_temp = copy.deepcopy(target)\n",
    "    source_temp.paint_uniform_color([1.0, 0.0, 0.0])  # red\n",
    "    target_temp.paint_uniform_color([0.0, 0.5, 1.0])  # blue\n",
    "    source_temp.transform(transformation)\n",
    "    o3d.visualization.draw_geometries([source_temp, target_temp])\n",
    "\n",
    "\n",
    "def global_registration_ransac(src_down, tgt_down, src_fpfh, tgt_fpfh, voxel_size: float):\n",
    "    \"\"\"\n",
    "    RANSAC-based global registration on FPFH feature matches.\n",
    "    \"\"\"\n",
    "    distance_threshold = voxel_size * 1.5\n",
    "\n",
    "    result = o3d.pipelines.registration.registration_ransac_based_on_feature_matching(\n",
    "        src_down, tgt_down,\n",
    "        src_fpfh, tgt_fpfh,\n",
    "        mutual_filter=True, # ensure src/tar NN corespondance\n",
    "        max_correspondence_distance=distance_threshold,\n",
    "        estimation_method=o3d.pipelines.registration.TransformationEstimationPointToPoint(False), # rigid transform\n",
    "        ransac_n=4, # sample point pairs to solve T \n",
    "        checkers=[ # pre filter\n",
    "            # Enforce similar edge length ratios to reject degenerate matches\n",
    "            o3d.pipelines.registration.CorrespondenceCheckerBasedOnEdgeLength(0.9),\n",
    "            # Reject matches with too large geometric distance\n",
    "            o3d.pipelines.registration.CorrespondenceCheckerBasedOnDistance(distance_threshold),\n",
    "        ],\n",
    "        criteria=o3d.pipelines.registration.RANSACConvergenceCriteria(\n",
    "            max_iteration=100000,  \n",
    "            confidence=0.999\n",
    "        )\n",
    "    )\n",
    "    return result\n",
    "\n",
    "\n",
    "def refine_registration_icp(src_full, tgt_full, init_T, voxel_size, use_point_to_plane=False):\n",
    "    \"\"\"\n",
    "    ICP refinement (point-to-plane is usually better if normals are reliable).\n",
    "    \"\"\"\n",
    "    # Set a tighter threshold than RANSAC\n",
    "    max_corr_dist = voxel_size * 1.0\n",
    "\n",
    "    if use_point_to_plane:\n",
    "        # Normals are required for point-to-plane ICP (target normals are used)\n",
    "        tgt_full.estimate_normals(\n",
    "            o3d.geometry.KDTreeSearchParamHybrid(radius=voxel_size * 2.0, max_nn=30)\n",
    "        )\n",
    "        src_full.estimate_normals(\n",
    "            o3d.geometry.KDTreeSearchParamHybrid(radius=voxel_size * 2.0, max_nn=30)\n",
    "        )\n",
    "        estimation = o3d.pipelines.registration.TransformationEstimationPointToPlane()\n",
    "    else:\n",
    "        estimation = o3d.pipelines.registration.TransformationEstimationPointToPoint()\n",
    "\n",
    "    result_icp = o3d.pipelines.registration.registration_icp(\n",
    "        src_full, tgt_full,\n",
    "        max_corr_dist,\n",
    "        init_T,\n",
    "        estimation_method=estimation,\n",
    "        criteria=o3d.pipelines.registration.ICPConvergenceCriteria(max_iteration=80)\n",
    "    )\n",
    "    return result_icp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded:\n",
      "  ref: PointCloud with 1349372 points.\n",
      "  mov: PointCloud with 1711170 points.\n",
      "Downsampled:\n",
      "  ref_down: PointCloud with 57450 points.\n",
      "  mov_down: PointCloud with 59566 points.\n",
      "FPFH dims: (33, 57450) (33, 59566)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ref_path = \"../Data/ICP_test/ref_Tete_D.ply\"\n",
    "mov_path = \"../Data/ICP_test/move_Dragon_01_Transform.ply\"\n",
    "\n",
    "ref_pcd = o3d.io.read_point_cloud(ref_path)\n",
    "mov_pcd = o3d.io.read_point_cloud(mov_path)\n",
    "\n",
    "print(\"Loaded:\")\n",
    "print(\"  ref:\", ref_pcd)\n",
    "print(\"  mov:\", mov_pcd)\n",
    "\n",
    "\n",
    "voxel_size = 5.0\n",
    "\n",
    "# 1) Preprocess for FPFH + RANSAC on downsampled point clouds\n",
    "ref_down, ref_fpfh = preprocess_point_cloud(ref_pcd, voxel_size)\n",
    "mov_down, mov_fpfh = preprocess_point_cloud(mov_pcd, voxel_size)\n",
    "\n",
    "print(\"Downsampled:\")\n",
    "print(\"  ref_down:\", ref_down)\n",
    "print(\"  mov_down:\", mov_down)\n",
    "print(\"FPFH dims:\", ref_fpfh.data.shape, mov_fpfh.data.shape)  # (33, N)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANSAC result:\n",
      "  fitness: 0.7830305879192828\n",
      "  inlier_rmse: 2.664491435886807\n",
      "  T_ransac:\n",
      " [[ 9.55147585e-01  2.95106543e-01 -2.46011912e-02 -4.31887167e+01]\n",
      " [-2.95069755e-01  9.41413079e-01 -1.63325612e-01 -9.31767843e+02]\n",
      " [-2.50385735e-02  1.63259131e-01  9.86265444e-01 -4.12658185e+01]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# 2) Global registration (RANSAC)\n",
    "result_ransac = global_registration_ransac(mov_down, ref_down, mov_fpfh, ref_fpfh, voxel_size)\n",
    "print(\"\\nRANSAC result:\")\n",
    "print(\"  fitness:\", result_ransac.fitness)\n",
    "print(\"  inlier_rmse:\", result_ransac.inlier_rmse)\n",
    "print(\"  T_ransac:\\n\", result_ransac.transformation)\n",
    "\n",
    "# Visualize coarse alignment\n",
    "draw_registration_result(mov_down, ref_down, result_ransac.transformation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ICP result:\n",
      "  fitness: 0.7907022680388273\n",
      "  inlier_rmse: 1.2211446515516153\n",
      "  T_icp:\n",
      " [[ 9.54329399e-01  2.97668491e-01 -2.54728865e-02 -4.74875924e+01]\n",
      " [-2.97744996e-01  9.40625150e-01 -1.63009950e-01 -9.32482530e+02]\n",
      " [-2.45624882e-02  1.63149612e-01  9.86295538e-01 -4.11525286e+01]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# 3) ICP refinement on full-resolution clouds (or you can use downsampled first)\n",
    "result_icp = refine_registration_icp(mov_pcd, ref_pcd, result_ransac.transformation, voxel_size, use_point_to_plane=True)\n",
    "print(\"\\nICP result:\")\n",
    "print(\"  fitness:\", result_icp.fitness)\n",
    "print(\"  inlier_rmse:\", result_icp.inlier_rmse)\n",
    "print(\"  T_icp:\\n\", result_icp.transformation)\n",
    "\n",
    "# Visualize refined alignment\n",
    "draw_registration_result(mov_pcd, ref_pcd, result_icp.transformation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved: mov_aligned_by_fpfh_ransac_icp_p2p.ply\n"
     ]
    }
   ],
   "source": [
    "# 4) Save transformed moving point cloud (optional)\n",
    "mov_aligned = copy.deepcopy(mov_pcd)\n",
    "mov_aligned.transform(result_icp.transformation)\n",
    "o3d.io.write_point_cloud(\"../Data/FPFH/mov_aligned_by_fpfh_ransac_icp_p2p.ply\", mov_aligned)\n",
    "print(\"\\nSaved:\", \"mov_aligned_by_fpfh_ransac_icp_p2p.ply\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison\n",
    "[16:12:34] [ComputeDistances] Mean distance = 11.5045 / std deviation = 26.9021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nn_distances(source: o3d.geometry.PointCloud, target: o3d.geometry.PointCloud) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute nearest neighbor distances from each point in 'source' to 'target'.\n",
    "    Returns:\n",
    "        dists: (N,) array of Euclidean distances.\n",
    "    \"\"\"\n",
    "    target_kd = o3d.geometry.KDTreeFlann(target)\n",
    "    src_pts = np.asarray(source.points)\n",
    "\n",
    "    dists = np.empty(len(src_pts), dtype=np.float64)\n",
    "    for i, p in enumerate(src_pts):\n",
    "        # 1-NN search\n",
    "        _, idx, dist2 = target_kd.search_knn_vector_3d(p, 1)\n",
    "        dists[i] = np.sqrt(dist2[0])\n",
    "    return dists\n",
    "\n",
    "\n",
    "def registration_metrics(source_aligned: o3d.geometry.PointCloud,\n",
    "                         target: o3d.geometry.PointCloud,\n",
    "                         thresholds=(5.0, 10.0),\n",
    "                         percentiles=(50, 90, 95)) -> dict:\n",
    "    \"\"\"\n",
    "    Compute overlap-aware metrics for registration evaluation.\n",
    "\n",
    "    Metrics:\n",
    "      - median / P90 / P95 distances\n",
    "      - coverage@tau: ratio of points with dist < tau\n",
    "      - trimmed_rmse@tau: RMSE computed only on points with dist < tau\n",
    "      - mean / std (for reference)\n",
    "    \"\"\"\n",
    "    d = nn_distances(source_aligned, target)\n",
    "\n",
    "    out = {\n",
    "        \"mean\": float(d.mean()),\n",
    "        \"std\": float(d.std()),\n",
    "        \"median\": float(np.percentile(d, 50)),\n",
    "    }\n",
    "    for p in percentiles:\n",
    "        out[f\"p{p}\"] = float(np.percentile(d, p))\n",
    "\n",
    "    for tau in thresholds:\n",
    "        inliers = d < tau\n",
    "        coverage = float(inliers.mean())\n",
    "        if inliers.any():\n",
    "            trimmed_rmse = float(np.sqrt(np.mean(d[inliers] ** 2)))\n",
    "            trimmed_mean = float(d[inliers].mean())\n",
    "        else:\n",
    "            trimmed_rmse = float(\"nan\")\n",
    "            trimmed_mean = float(\"nan\")\n",
    "        out[f\"coverage@{tau}\"] = coverage\n",
    "        out[f\"trimmed_mean@{tau}\"] = trimmed_mean\n",
    "        out[f\"trimmed_rmse@{tau}\"] = trimmed_rmse\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def symmetric_chamfer(source_aligned: o3d.geometry.PointCloud,\n",
    "                      target: o3d.geometry.PointCloud) -> float:\n",
    "    \"\"\"\n",
    "    Symmetric Chamfer distance (mean NN distance both directions).\n",
    "    \"\"\"\n",
    "    d_st = nn_distances(source_aligned, target).mean()\n",
    "    d_ts = nn_distances(target, source_aligned).mean()\n",
    "    return float(d_st + d_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original not registered evaluation\n",
      "{'mean': 28.790589237437796, 'std': 21.611957785388448, 'median': 24.060411166186718, 'p50': 24.060411166186718, 'p90': 61.29547679266786, 'p95': 71.37153082536858, 'coverage@5.0': 0.1196292595124973, 'trimmed_mean@5.0': 2.4777668386423106, 'trimmed_rmse@5.0': 2.83796814333311, 'coverage@10.0': 0.22594189940216344, 'trimmed_mean@10.0': 4.844387508719292, 'trimmed_rmse@10.0': 5.637451842177725}\n",
      "Symmetric Chamfer: 54.16179091590028\n"
     ]
    }
   ],
   "source": [
    "ref_path = \"../Data/ICP_test/ref_Tete_D.ply\"\n",
    "mov_path = \"../Data/ICP_test/move_Dragon_01_Transform.ply\"\n",
    "src = o3d.io.read_point_cloud(mov_path)\n",
    "tgt = o3d.io.read_point_cloud(ref_path)\n",
    "\n",
    "# src_aligned = copy.deepcopy(src)\n",
    "# src_aligned.transform(T_total)\n",
    "\n",
    "metrics = registration_metrics(src, tgt, thresholds=(5.0, 10.0))\n",
    "print(\"Original not registered evaluation\")\n",
    "print(metrics)\n",
    "\n",
    "print(\"Symmetric Chamfer:\", symmetric_chamfer(src, tgt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual landmark + ICP evaluation\n",
      "{'mean': 11.900873394379946, 'std': 26.68532721206653, 'median': 0.8916812282821849, 'p50': 0.8916812282821849, 'p90': 52.385716913856236, 'p95': 79.81466731250123, 'coverage@5.0': 0.7907560324222608, 'trimmed_mean@5.0': 0.9568073840270434, 'trimmed_rmse@5.0': 1.224656404608553, 'coverage@10.0': 0.8167061133610337, 'trimmed_mean@10.0': 1.1507352380483076, 'trimmed_rmse@10.0': 1.7607851593499064}\n",
      "Symmetric Chamfer: 16.668003987122248\n"
     ]
    }
   ],
   "source": [
    "ref_path = \"../Data/ICP_test/ref_Tete_D.ply\"\n",
    "mov_path = \"../Data/ICP_test/dragon_after_icp_python.ply\"\n",
    "src = o3d.io.read_point_cloud(mov_path)\n",
    "tgt = o3d.io.read_point_cloud(ref_path)\n",
    "\n",
    "# src_aligned = copy.deepcopy(src)\n",
    "# src_aligned.transform(T_total)\n",
    "\n",
    "metrics = registration_metrics(src, tgt, thresholds=(5.0, 10.0))\n",
    "print(\"Manual landmark + ICP evaluation\")\n",
    "print(metrics)\n",
    "\n",
    "print(\"Symmetric Chamfer:\", symmetric_chamfer(src, tgt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPFH + RANSAC + ICP evaluation\n",
      "{'mean': 11.919651183532961, 'std': 26.723596740054912, 'median': 0.8878449859671382, 'p50': 0.8878449859671382, 'p90': 52.503750839545056, 'p95': 79.93291604220614, 'coverage@5.0': 0.7907022680388273, 'trimmed_mean@5.0': 0.953115089069285, 'trimmed_rmse@5.0': 1.2211446515516156, 'coverage@10.0': 0.8165477421880936, 'trimmed_mean@10.0': 1.1466224471712876, 'trimmed_rmse@10.0': 1.7575508328904914}\n",
      "Symmetric Chamfer: 16.678766929614426\n"
     ]
    }
   ],
   "source": [
    "ref_path = \"../Data/ICP_test/ref_Tete_D.ply\"\n",
    "mov_path = \"../Data/FPFH/mov_aligned_by_fpfh_ransac_icp_p2p.ply\"\n",
    "src = o3d.io.read_point_cloud(mov_path)\n",
    "tgt = o3d.io.read_point_cloud(ref_path)\n",
    "\n",
    "# src_aligned = copy.deepcopy(src)\n",
    "# src_aligned.transform(T_total)\n",
    "\n",
    "metrics = registration_metrics(src, tgt, thresholds=(5.0, 10.0))\n",
    "print(\"FPFH + RANSAC + ICP evaluation\")\n",
    "print(metrics)\n",
    "\n",
    "print(\"Symmetric Chamfer:\", symmetric_chamfer(src, tgt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpinNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(\"./SpinNet\"))\n",
    "from network.SpinNet import Descriptor_Net\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "\n",
    "\n",
    "def load_ckpt_strip_module(ckpt_path: str, map_location=\"cpu\"):\n",
    "    ckpt = torch.load(ckpt_path, map_location=map_location)\n",
    "\n",
    "    # 情况1：直接就是 state_dict\n",
    "    state_dict = ckpt\n",
    "\n",
    "    # 情况2：ckpt 是 dict，里面有 'state_dict' 或 'model'\n",
    "    if isinstance(ckpt, dict):\n",
    "        if \"state_dict\" in ckpt:\n",
    "            state_dict = ckpt[\"state_dict\"]\n",
    "        elif \"model\" in ckpt:\n",
    "            state_dict = ckpt[\"model\"]\n",
    "        elif \"net\" in ckpt:\n",
    "            state_dict = ckpt[\"net\"]\n",
    "\n",
    "    # 去掉 'module.' 前缀\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        new_k = k[7:] if k.startswith(\"module.\") else k\n",
    "        new_state_dict[new_k] = v\n",
    "    return new_state_dict\n",
    "\n",
    "\n",
    "def build_spinnet_model(ckpt_path,\n",
    "                        des_r=0.30, rad_n=9, azi_n=80, ele_n=40,\n",
    "                        voxel_r=0.04, voxel_sample=30,\n",
    "                        dataset=\"3DMatch\",\n",
    "                        device=\"cuda:0\"):\n",
    "\n",
    "    model = Descriptor_Net(des_r, rad_n, azi_n, ele_n, voxel_r, voxel_sample, dataset)\n",
    "\n",
    "    sd = load_ckpt_strip_module(ckpt_path, map_location=\"cpu\")\n",
    "\n",
    "    # strict=True 一般就能过；如果你改过网络结构才需要 strict=False\n",
    "    model.load_state_dict(sd, strict=True)\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    return model\n",
    "def spinnet_features_for_pcd(pcd_down: o3d.geometry.PointCloud,\n",
    "                             model: torch.nn.Module,\n",
    "                             patch_radius: float,\n",
    "                             N: int = 2048,\n",
    "                             batch_size: int = 64,\n",
    "                             device: str = 'cuda:0'):\n",
    "    \"\"\"\n",
    "    For each point in pcd_down, build a local patch (N,3), run SpinNet, return Open3D Feature (32, num_pts).\n",
    "    \"\"\"\n",
    "    pts = np.asarray(pcd_down.points).astype(np.float32)\n",
    "    num_pts = pts.shape[0]\n",
    "\n",
    "    # KDTree for neighborhood query\n",
    "    kdtree = o3d.geometry.KDTreeFlann(pcd_down)\n",
    "\n",
    "    desc_list = []\n",
    "    # Build patches in chunks\n",
    "    patches_buf = []\n",
    "    idx_buf = []\n",
    "\n",
    "    for i in range(num_pts):\n",
    "        center = pts[i]\n",
    "\n",
    "        # radius search\n",
    "        _, idxs, _ = kdtree.search_radius_vector_3d(center, patch_radius)\n",
    "        if len(idxs) < 5:\n",
    "            # fallback: if neighborhood is too small, just repeat center\n",
    "            patch = np.repeat(center[None, :], N, axis=0)\n",
    "        else:\n",
    "            neigh = pts[np.asarray(idxs, dtype=np.int64)]\n",
    "\n",
    "            # sample to fixed N\n",
    "            if neigh.shape[0] >= N:\n",
    "                sel = np.random.choice(neigh.shape[0], N, replace=False)\n",
    "                patch = neigh[sel]\n",
    "            else:\n",
    "                # pad by resampling with replacement\n",
    "                sel = np.random.choice(neigh.shape[0], N, replace=True)\n",
    "                patch = neigh[sel]\n",
    "\n",
    "            # IMPORTANT: ensure the last point is the center (SpinNet uses input[:, -1, :] as center)\n",
    "            patch[-1] = center\n",
    "\n",
    "        patches_buf.append(patch)\n",
    "        idx_buf.append(i)\n",
    "\n",
    "        # run a batch\n",
    "        if len(patches_buf) == batch_size or i == num_pts - 1:\n",
    "            batch = torch.from_numpy(np.stack(patches_buf, axis=0)).to(device=device, dtype=torch.float32)  # (B,N,3)\n",
    "            with torch.no_grad():\n",
    "                out = model(batch)                   # (B,32,1,1)\n",
    "                out = out.view(out.shape[0], -1)     # (B,32)\n",
    "                out = F.normalize(out, p=2, dim=1)   # (B,32)\n",
    "            desc_list.append(out.detach().cpu().numpy())\n",
    "            patches_buf = []\n",
    "            idx_buf = []\n",
    "\n",
    "    desc = np.concatenate(desc_list, axis=0)  # (num_pts, 32)\n",
    "\n",
    "    feat = o3d.pipelines.registration.Feature()\n",
    "    feat.data = desc.T  # Open3D expects (dim, num_pts)\n",
    "    return feat\n",
    "    \n",
    "def neighborhood_stats(pcd_down, radius, max_check=5000):\n",
    "    pts = np.asarray(pcd_down.points)\n",
    "    kdtree = o3d.geometry.KDTreeFlann(pcd_down)\n",
    "    n = pts.shape[0]\n",
    "    idxs = np.random.choice(n, size=min(n, max_check), replace=False)\n",
    "\n",
    "    counts = []\n",
    "    t0 = time.time()\n",
    "    for i in idxs:\n",
    "        _, nn, _ = kdtree.search_radius_vector_3d(pts[i], radius)\n",
    "        counts.append(len(nn))\n",
    "    t = time.time() - t0\n",
    "    counts = np.array(counts)\n",
    "    print(f\"[radius={radius}] checked {len(idxs)} points in {t:.2f}s\")\n",
    "    print(\"  nn count: min / median / p90 / max =\",\n",
    "          counts.min(), np.median(counts), np.percentile(counts, 90), counts.max())\n",
    "    return counts\n",
    "\n",
    "def spinnet_features_for_pcd_profiled(\n",
    "    pcd_down: o3d.geometry.PointCloud,\n",
    "    model: torch.nn.Module,\n",
    "    patch_radius: float,\n",
    "    N: int = 2048,\n",
    "    batch_size: int = 64,\n",
    "    device: str = \"cuda:0\",\n",
    "    warmup_batches: int = 2,\n",
    "):\n",
    "    pts = np.asarray(pcd_down.points).astype(np.float32)\n",
    "    num_pts = pts.shape[0]\n",
    "    kdtree = o3d.geometry.KDTreeFlann(pcd_down)\n",
    "\n",
    "    # timers\n",
    "    t_kdtree = 0.0\n",
    "    t_patch  = 0.0\n",
    "    t_stack  = 0.0\n",
    "    t_fwd    = 0.0\n",
    "\n",
    "    desc_list = []\n",
    "    patches_buf = []\n",
    "\n",
    "    # optional warmup (use first few points)\n",
    "    if \"cuda\" in device and torch.cuda.is_available():\n",
    "        for _ in range(warmup_batches):\n",
    "            # build a tiny dummy batch\n",
    "            B = min(batch_size, max(1, num_pts))\n",
    "            dummy = torch.randn(B, N, 3, device=device, dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                torch.cuda.synchronize()\n",
    "                _ = model(dummy)\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "    t_total0 = time.perf_counter()\n",
    "\n",
    "    for i in range(num_pts):\n",
    "        center = pts[i]\n",
    "\n",
    "        # 1) KDTree radius search timing\n",
    "        t0 = time.perf_counter()\n",
    "        _, idxs, _ = kdtree.search_radius_vector_3d(center, patch_radius)\n",
    "        t_kdtree += time.perf_counter() - t0\n",
    "\n",
    "        # 2) Patch build timing (sampling/padding)\n",
    "        t0 = time.perf_counter()\n",
    "        if len(idxs) < 5:\n",
    "            patch = np.repeat(center[None, :], N, axis=0)\n",
    "        else:\n",
    "            neigh = pts[np.asarray(idxs, dtype=np.int64)]\n",
    "            if neigh.shape[0] >= N:\n",
    "                sel = np.random.choice(neigh.shape[0], N, replace=False)\n",
    "            else:\n",
    "                sel = np.random.choice(neigh.shape[0], N, replace=True)\n",
    "            patch = neigh[sel]\n",
    "            patch[-1] = center\n",
    "        t_patch += time.perf_counter() - t0\n",
    "\n",
    "        patches_buf.append(patch)\n",
    "\n",
    "        # 3) batch forward timing\n",
    "        if len(patches_buf) == batch_size or i == num_pts - 1:\n",
    "            t0 = time.perf_counter()\n",
    "            batch_np = np.stack(patches_buf, axis=0)   # (B,N,3)\n",
    "            t_stack += time.perf_counter() - t0\n",
    "\n",
    "            batch = torch.from_numpy(batch_np).to(device=device, dtype=torch.float32)\n",
    "\n",
    "            if \"cuda\" in device and torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            t0 = time.perf_counter()\n",
    "            with torch.no_grad():\n",
    "                out = model(batch)               # (B,32,1,1) or similar\n",
    "                out = out.view(out.shape[0], -1) # (B,32)\n",
    "                out = F.normalize(out, p=2, dim=1)\n",
    "            if \"cuda\" in device and torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            t_fwd += time.perf_counter() - t0\n",
    "\n",
    "            desc_list.append(out.detach().cpu().numpy())\n",
    "            patches_buf = []\n",
    "\n",
    "    t_total = time.perf_counter() - t_total0\n",
    "\n",
    "    desc = np.concatenate(desc_list, axis=0)  # (num_pts, 32)\n",
    "    feat = o3d.pipelines.registration.Feature()\n",
    "    feat.data = desc.T  # (32, num_pts)\n",
    "\n",
    "    profile = {\n",
    "        \"num_pts\": num_pts,\n",
    "        \"total_s\": t_total,\n",
    "        \"per_point_ms\": (t_total / max(1, num_pts)) * 1000.0,\n",
    "        \"kdtree_s\": t_kdtree,\n",
    "        \"patch_build_s\": t_patch,\n",
    "        \"stack_s\": t_stack,\n",
    "        \"forward_s\": t_fwd,\n",
    "        \"kdtree_ms_per_pt\": (t_kdtree / max(1, num_pts)) * 1000.0,\n",
    "        \"patch_ms_per_pt\": (t_patch / max(1, num_pts)) * 1000.0,\n",
    "        \"stack_ms_per_pt\": (t_stack / max(1, num_pts)) * 1000.0,\n",
    "        \"fwd_ms_per_pt\": (t_fwd / max(1, num_pts)) * 1000.0,\n",
    "    }\n",
    "    return feat, profile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diag = 1331.7356338057582\n",
      "After scale, diag = 1.3317356338051947\n"
     ]
    }
   ],
   "source": [
    "ref_path = \"Data/ICP_test/ref_Tete_D.ply\"\n",
    "mov_path = \"Data/ICP_test/move_Dragon_01_Transform.ply\"\n",
    "\n",
    "ref_pcd = o3d.io.read_point_cloud(ref_path)\n",
    "mov_pcd = o3d.io.read_point_cloud(mov_path)\n",
    "\n",
    "\n",
    "bbox = ref_pcd.get_axis_aligned_bounding_box()\n",
    "diag = np.linalg.norm(bbox.get_extent())\n",
    "print(\"diag =\", diag) # Verify the len unity\n",
    "\n",
    "# 0) Unity transfer\n",
    "scale = 1e-3\n",
    "ref_pcd.scale(scale, center=ref_pcd.get_center())\n",
    "mov_pcd.scale(scale, center=mov_pcd.get_center())\n",
    "\n",
    "# 1) downsample（unity：m）\n",
    "voxel_size = 0.005  # 5mm = 0.005m\n",
    "ref_down = ref_pcd.voxel_down_sample(voxel_size)\n",
    "mov_down = mov_pcd.voxel_down_sample(voxel_size)\n",
    "\n",
    "ref_down, _ = ref_down.remove_statistical_outlier(nb_neighbors=20, std_ratio=2.0)\n",
    "mov_down, _ = mov_down.remove_statistical_outlier(nb_neighbors=20, std_ratio=2.0)\n",
    "\n",
    "bbox = ref_pcd.get_axis_aligned_bounding_box()\n",
    "diag = np.linalg.norm(bbox.get_extent())\n",
    "print(\"After scale, diag =\", diag) # Verify the len unity\n",
    "\n",
    "print(\"ref_down points:\", np.asarray(ref_down.points).shape[0])\n",
    "print(\"mov_down points:\", np.asarray(mov_down.points).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4027753/2108995958.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(ckpt_path, map_location=map_location)\n"
     ]
    }
   ],
   "source": [
    "# 2) Build SpinNet model\n",
    "ckpt_path = \"SpinNet/pre-trained_models/3DMatch_best.pkl\"   \n",
    "model = build_spinnet_model(ckpt_path,\n",
    "                            des_r=0.30, rad_n=9, azi_n=80, ele_n=40,\n",
    "                            voxel_r=0.04, voxel_sample=30,\n",
    "                            dataset=\"3DMatch\",\n",
    "                            device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded \n",
      "  ref_down: PointCloud with 57451 points.\n",
      "  mov_down: PointCloud with 59565 points.\n",
      "[radius=0.3] checked 5000 points in 10.37s\n",
      "  nn count: min / median / p90 / max = 4800 24746.5 32293.500000000007 36282\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([32898, 23067, 28527, ..., 32842, 23483, 26997])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.5)\n",
    "print(\"Loaded \")\n",
    "print(\"  ref_down:\", ref_down)\n",
    "print(\"  mov_down:\", mov_down)\n",
    "\n",
    "neighborhood_stats(ref_down, radius=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated total seconds on full ref_down: 212.8129515532977\n"
     ]
    }
   ],
   "source": [
    "# 2.5) Time test\n",
    "idx = np.random.choice(len(ref_down.points), 2000, replace=False)\n",
    "ref_sub = ref_down.select_by_index(idx)\n",
    "\n",
    "_, prof_sub = spinnet_features_for_pcd_profiled(\n",
    "    ref_sub, model, patch_radius=0.30, N=2048, batch_size=32, device=\"cuda:0\"\n",
    ")\n",
    "\n",
    "est_total = prof_sub[\"per_point_ms\"] * len(ref_down.points) / 1000.0\n",
    "print(\"estimated total seconds on full ref_down:\", est_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_pts': 2000, 'total_s': 7.408502952195704, 'per_point_ms': 3.704251476097852, 'kdtree_s': 0.1374448137357831, 'patch_build_s': 0.21178276371210814, 'stack_s': 0.019267170690000057, 'forward_s': 7.004434240050614, 'kdtree_ms_per_pt': 0.06872240686789155, 'patch_ms_per_pt': 0.10589138185605407, 'stack_ms_per_pt': 0.009633585345000029, 'fwd_ms_per_pt': 3.502217120025307}\n"
     ]
    }
   ],
   "source": [
    "print(prof_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) SpinNet feature（patch_radius：m）\n",
    "ref_feat, ref_prof = spinnet_features_for_pcd_profiled(ref_down, model, patch_radius=0.30, N=2048, batch_size=48, device=\"cuda:0\")\n",
    "\n",
    "\n",
    "#ref_feat = spinnet_features_for_pcd(ref_down, model, patch_radius=0.30, N=2048, batch_size=48, device=\"cuda:0\")\n",
    "#mov_feat = spinnet_features_for_pcd(mov_down, model, patch_radius=0.30, N=2048, batch_size=48, device=\"cuda:0\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_pts': 57451, 'total_s': 366.79874674696475, 'per_point_ms': 6.3845493855105175, 'kdtree_s': 116.37929208111018, 'patch_build_s': 55.9014504281804, 'stack_s': 0.3908678153529763, 'forward_s': 193.26412159670144, 'kdtree_ms_per_pt': 2.0257139489497167, 'patch_ms_per_pt': 0.9730283272385232, 'stack_ms_per_pt': 0.00680349890085423, 'fwd_ms_per_pt': 3.363981855784955}\n"
     ]
    }
   ],
   "source": [
    "print(ref_prof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mov_feat, mov_prof = spinnet_features_for_pcd_profiled(mov_down, model, patch_radius=0.30, N=2048, batch_size=48, device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_pts': 59565, 'total_s': 353.89798430912197, 'per_point_ms': 5.941374705097322, 'kdtree_s': 107.84113709721714, 'patch_build_s': 52.47581277042627, 'stack_s': 0.3811010494828224, 'forward_s': 192.4380912426859, 'kdtree_ms_per_pt': 1.8104782522826683, 'patch_ms_per_pt': 0.8809840136057462, 'stack_ms_per_pt': 0.006398070166756021, 'fwd_ms_per_pt': 3.23072427168112}\n"
     ]
    }
   ],
   "source": [
    "print(mov_prof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m[Open3D WARNING] Too few correspondences (514) after mutual filter, fall back to original correspondences.\u001b[0;m\n",
      "\n",
      "RANSAC result:\n",
      "  fitness: 0.6604213883992277\n",
      "  inlier_rmse: 0.004330643965148919\n",
      "  T_ransac:\n",
      " [[ 9.43375853e-01  3.31017676e-01 -2.16632845e-02 -3.96642037e+01]\n",
      " [-3.29924565e-01  9.29454455e-01 -1.65118739e-01 -9.87221529e+02]\n",
      " [-3.45221849e-02  1.62916281e-01  9.86035752e-01 -5.37294406e+01]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# 4) RANSAC \n",
    "result_ransac = global_registration_ransac(mov_down, ref_down, mov_feat, ref_feat, voxel_size)\n",
    "print(\"\\nRANSAC result:\")\n",
    "print(\"  fitness:\", result_ransac.fitness)\n",
    "print(\"  inlier_rmse:\", result_ransac.inlier_rmse)\n",
    "print(\"  T_ransac:\\n\", result_ransac.transformation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ICP result:\n",
      "  fitness: 0.7907040212252435\n",
      "  inlier_rmse: 0.0012213668000526324\n",
      "  T_icp:\n",
      " [[ 9.54331532e-01  2.97661486e-01 -2.54748046e-02 -5.88602838e+01]\n",
      " [-2.97738628e-01  9.40628721e-01 -1.63000974e-01 -9.51272785e+02]\n",
      " [-2.45567794e-02  1.63141803e-01  9.86296972e-01 -4.53309096e+01]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# 5) ICP\n",
    "result_icp = refine_registration_icp(mov_pcd, ref_pcd, result_ransac.transformation, voxel_size, use_point_to_plane=False)\n",
    "print(\"\\nICP result:\")\n",
    "print(\"  fitness:\", result_icp.fitness)\n",
    "print(\"  inlier_rmse:\", result_icp.inlier_rmse)\n",
    "print(\"  T_icp:\\n\", result_icp.transformation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved: mov_aligned_by_SpinNet_ransac_icp_p2p.ply\n"
     ]
    }
   ],
   "source": [
    "# 4) Save transformed moving point cloud (optional)\n",
    "mov_aligned = copy.deepcopy(mov_pcd)\n",
    "mov_aligned.transform(result_icp.transformation)\n",
    "o3d.io.write_point_cloud(\"Data/SpinNet/mov_aligned_by_SpinNet_ransac_icp_p2p.ply\", mov_aligned)\n",
    "print(\"\\nSaved:\", \"mov_aligned_by_SpinNet_ransac_icp_p2p.ply\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_registration(pcd, scale=1e-3):\n",
    "    p = o3d.geometry.PointCloud(pcd)  \n",
    "    c = p.get_center()\n",
    "    p.scale(scale, center=c)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean': 0.011918822987517803, 'std': 0.026722915787976767, 'median': 0.0008861635487700434, 'p50': 0.0008861635487700434, 'p90': 0.05249955660834527, 'p95': 0.07992664360168701, 'coverage@0.005': 0.7907040212252435, 'trimmed_mean@0.005': 0.0009527310011387557, 'trimmed_rmse@0.005': 0.0012213667999690684, 'coverage@0.01': 0.8165454046062051, 'trimmed_mean@0.01': 0.0011462205703362967, 'trimmed_rmse@0.01': 0.0017576162087935656}\n",
      "Symmetric Chamfer: 0.016677589187423562\n"
     ]
    }
   ],
   "source": [
    "ref_path = \"Data/ICP_test/ref_Tete_D.ply\"\n",
    "mov_path = \"Data/SpinNet/mov_aligned_by_SpinNet_ransac_icp_p2p.ply\"\n",
    "\n",
    "\n",
    "src = o3d.io.read_point_cloud(mov_path)\n",
    "tgt = o3d.io.read_point_cloud(ref_path)\n",
    "\n",
    "tgt = preprocess_for_registration(tgt)\n",
    "\n",
    "metrics = registration_metrics(src, tgt, thresholds=(0.005, 0.010))\n",
    "print(metrics)\n",
    "print(\"Symmetric Chamfer:\", symmetric_chamfer(src, tgt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training SpinNet\n",
    "load training files ../../data/3DMatch/patches/train/train_anc&pos_20_2048_2000.pkl\n",
    "Epoch 19: Loss 1.0435524266898795, time 10648.4401s\n",
    "load training files ../../data/3DMatch/patches/val/val_anc&pos_2_2048_4000.pkl\n",
    "load training files ../../data/3DMatch/patches/val/val_anc&pos_2_2048_6000.pkl\n",
    "load training files ../../data/3DMatch/patches/val/val_anc&pos_2_2048_8000.pkl\n",
    "load training files ../../data/3DMatch/patches/val/val_anc&pos_2_2048_10000.pkl\n",
    "load training files ../../data/3DMatch/patches/val/val_anc&pos_2_2048_12000.pkl\n",
    "load training files ../../data/3DMatch/patches/val/val_anc&pos_2_2048_14000.pkl\n",
    "load training files ../../data/3DMatch/patches/val/val_anc&pos_2_2048_16000.pkl\n",
    "load training files ../../data/3DMatch/patches/val/val_anc&pos_2_2048_17043.pkl\n",
    "load training files ../../data/3DMatch/patches/val/val_anc&pos_2_2048_2000.pkl\n",
    "Evaluation: Epoch 19: Loss 1.2269546277540968\n",
    "Avg one epoch time: 10514.17, total 20 epochs time: 214677.52\n",
    "Training finish!... save training results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare FPFH + ICP vs SpinNet + ICP on different downsample keypoints "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spinnet_cuda12)",
   "language": "python",
   "name": "spinnet_cuda12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
